[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)
# ComfyUI HunyuanVideo-1.5 Plugin

A ComfyUI plugin based on **HunyuanVideo-1.5**, offering both simplified and complete node sets for quick usage or deep workflow customization.

---

## âœ¨ Features

- **Simplified Nodes**: Includes only the core `HyVideo15ModelLoader` and `HyVideo15I2VSampler` nodes, ideal for quick testing and result preview.
- **Complete Nodes**: Provides more finely split nodes for detailed workflow adjustments and replacements.
- **Auto-Download Models**: Built-in automatic model download; no need to manually prepare model files (manual download also supported).

---

## ğŸ“¦ Installation

### Step 1: Install Dependencies
1. Install required libraries from `requirements.txt`:  `pip install -r requirements.txt`
2. Flash Attention: It's recommended to install Flash Attention for faster inference and reduced GPU memory consumption. Detailed installation instructions are available at Flash Attention.
### Step 2: Download Models
Choose one of the following methods to download the model files (including `hunyuanvideo-1.5` model, `text_encoder`, and `vision_encoder`):

#### Method 1: Auto-Download (Recommended)
Enable the **Auto-Download** option in the plugin when running a workflow. **Models will be automatically downloaded to the default path.When using the model's auto-download feature, please set the path of the model loading node to "None.**" The corresponding model will be automatically downloaded to the default directory (if it already exists, it will not be downloaded again). The next time you run the workflow, you can see the auto-downloaded model in the node's path options.

#### Method 2: Manual Download
Manually download the model file and place it in the model directory specified by the plugin. For detailed instructions, please refer to [checkpoints-download.md](checkpoints-download.md). (From the HunyuanVideo-1.5 open source project).

**The directory structure for model placement is as follows:**

```

models/
â”œâ”€â”€ clip_vision
â”‚   â””â”€â”€ hyvideo15
â”‚       â””â”€â”€ siglip
â”‚           â”œâ”€â”€ feature_extractor
â”‚           â”‚   â””â”€â”€ preprocessor_config.json
â”‚           â”œâ”€â”€ flux1-redux-dev.safetensors
â”‚           â”œâ”€â”€ image_embedder
â”‚           â”‚   â”œâ”€â”€ config.json
â”‚           â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚           â”œâ”€â”€ image_encoder
â”‚           â”‚   â”œâ”€â”€ config.json
â”‚           â”‚   â””â”€â”€ model.safetensors
â”‚           â”œâ”€â”€ LICENSE.md
â”‚           â”œâ”€â”€ model_index.json
â”‚           â”œâ”€â”€ README.md
â”‚           â””â”€â”€ redux.png
â”œâ”€â”€ diffusion_models
â”‚   â””â”€â”€hyvideo15
â”‚       â”œâ”€â”€ 1080p_sr_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 480p_i2v
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 480p_i2v_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 480p_t2v
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 480p_t2v_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_i2v
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_i2v_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_i2v_distilled_sparse
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_sr_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_t2v
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â”œâ”€â”€ 720p_t2v_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â””â”€â”€ 720p_t2v_distilled_sparse
â”‚           â”œâ”€â”€ config.json
â”‚           â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚   
â”œâ”€â”€ text_encoders
â”‚   â”œâ”€â”€ byt5-small
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ Glyph-SDXL-v2
â”‚   â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”‚   â”œâ”€â”€ color_idx.json
â”‚   â”‚   â”‚   â””â”€â”€ multilingual_10-lang_idx.json
â”‚   â”‚   â””â”€â”€ checkpoints
â”‚   â”‚       â””â”€â”€ byt5_model.pt
â”‚   â””â”€â”€ hyvideo15
â”‚       â””â”€â”€ llm
â”‚           â”œâ”€â”€ chat_template.json
â”‚           â”œâ”€â”€ config.json
â”‚           â”œâ”€â”€ generation_config.json
â”‚           â”œâ”€â”€ merges.txt
â”‚           â”œâ”€â”€ model-00001-of-00005.safetensors
â”‚           â”œâ”€â”€ model-00002-of-00005.safetensors
â”‚           â”œâ”€â”€ model-00003-of-00005.safetensors
â”‚           â”œâ”€â”€ model-00004-of-00005.safetensors
â”‚           â”œâ”€â”€ model-00005-of-00005.safetensors
â”‚           â”œâ”€â”€ model.safetensors.index.json
â”‚           â”œâ”€â”€ preprocessor_config.json
â”‚           â”œâ”€â”€ README.md
â”‚           â”œâ”€â”€ tokenizer_config.json
â”‚           â”œâ”€â”€ tokenizer.json
â”‚           â””â”€â”€ vocab.json
â”‚       
â”œâ”€â”€ upscale_models
â”‚   â””â”€â”€ hyvideo15
â”‚       â”œâ”€â”€ 1080p_sr_distilled
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚       â””â”€â”€ 720p_sr_distilled
â”‚           â”œâ”€â”€ config.json
â”‚           â””â”€â”€ diffusion_pytorch_model.safetensors
â”‚   
â””â”€â”€ vae
    â””â”€â”€ hyvideo15
        â”œâ”€â”€ config.json
        â””â”€â”€ diffusion_pytorch_model.safetensors
        
```

### Step 3: Import Workflow
1. Import the provided example workflow file (e.g., `simplified_I2V_workflow.json`) into ComfyUI.
2. Adjust necessary parameters, such as selecting the model path and loading the image.
3. Tweak parameters or replace nodes as needed (the complete node set allows for more flexible adjustments).

---

## ğŸ§© Node Description

### Simplified Nodes
- `HyVideo15ModelLoader`: Loads the HunyuanVideo-1.5 model.
- `HyVideo15I2VSampler`: Performs the video generation inference.

### Complete Nodes
In addition to the simplified functionality, the complete set includes the following split nodes:
- `HyVideoTextEncode`: Text encoder.
- `HyVideoVisionEncode`: Image encoder.
- Refer to the example workflows for more details.

---

## ğŸ›  Usage Tips

- Start with the **Simplified Workflow** for initial use to quickly verify results.
- Switch to the **Complete Nodes** for flexible assembly if you need to customize generation logic (e.g., replace encoders, adjust frame sequences).
- Ensure a stable internet connection for auto-download. If download fails, check the path or manually download the models.

---

## â“ FAQ

**Q: What should I do if auto-download fails?**  
A: Check your network connection, or manually download the models and place them in the corresponding subdirectories under `models/`.

**Q: How do I switch between Simplified and Complete versions?**  
A: Import the corresponding workflow file (e.g., `simplified_I2V_workflow.json` or `complete_I2V_workflow.json`) into ComfyUI. The nodes are grouped by version.

---

## ğŸ“„ License

This is an plugin based on the HunyuanVideo-1.5 model. Please comply with the relevant license agreement of the original model.